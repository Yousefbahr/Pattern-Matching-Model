# -*- coding: utf-8 -*-
"""hackathon (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LO1-5K4GZgJcvrB0W-j936yDpplv28d3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Softmax, Reshape, Embedding
from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Masking, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import load_model, Model

# sku 307 is not in the dataset file
data = pd.read_excel("data.xlsx", sheet_name="Dataset")
target = pd.read_excel("data.xlsx", sheet_name="Master File")

print(data.info())

seller_item_name = data["seller_item_name"]
sku_values = data["sku"]

# the input sequence will be 30

lengths = np.array([len(sen) for sen in seller_item_name])

print(np.sum(lengths > 30))

print(np.sum(lengths > 20))

plt.hist(lengths, bins=12)

Tx = 30

arabic_char = list("اﻷإﻹأآءئبتثجحخدذرزسشصضطظعغفقكلمنهويىة")
english_char = list("abcdefghijklmnopqrstuvwxyz")
symbols = list(".%/")
numbers = [str(num) for num in range(10)]
# arabic + symbols + english + numbers

# mapping characters to indices
index_to_char = {}
char_to_index = {}
for i, char in enumerate(arabic_char + symbols + english_char + numbers):
  index_to_char.update({i: char })
  char_to_index.update({char: i})

vocab_size = len(index_to_char)
print(max(seller_item_name, key=len))

def pre_process_strings(x, Tx, char_to_index, pad_value= -1):
  """
  x: list of strings
  Tx: sequence length
  char_to_index: a dict mapping characters to indices
  pad_value : the value to use in padding

  Processes a list of strings by applying regex then mapping the chars to indices

  Returns:
    indices: shape(x.shape[0], Tx)
  """

  for i, sentence in enumerate(x.copy()):
    # remove unwanted symbols
    new = re.sub(r"[^a-zA-Z0-9%\.\u0600-\u06FF\s]", " ", sentence)
    # remove tatweel symbol
    new = re.sub(r"\u0640", "" , new)
    # remove tashkeel
    new = re.sub(r"[\u064B-\u0652]", "", new)
    # remove arabic punctuation
    new = re.sub(r"[،؛؟“”‘’•]", "", new)
    # remove extra spaces
    new = re.sub(r"^\s+|\s+$|\s{2,}", " ", new).strip()
    new = re.sub(r"^\s+", " ", new)
    x[i] = new


  indices = np.full((len(x), Tx), pad_value)
  for i, sentence in enumerate(x):
    chars = list(sentence)
    for j, char in enumerate(chars):
      char = char.lower()
      # remove whitespace
      if char == ' ':
        continue
      if j < Tx:
        if char in char_to_index:
          indices[i, j] = char_to_index[char]


  return indices

def pre_process_dataset_training(X, Y,Tx, char_to_index, unique_classes = 500, num_pos_neg = 130):
  """
  X: array of input strings (samples) of shape(m,) | where m is number of samples
  Y: array of target strings of shape (m,) | where m is number of samples
  sku: array of sku of shape (m,)
  Tx: sequence length
  num_pos_neg: is the number of positive and negative samples for each sample
  char_to_index: a dict mapping all characters in vocab to their indices (0 to vocab_size)
  unique_classes: number of unqiue classes in the dataset

  returns 80 positve and negative samples for each sample
  for negative samples randomly choose another class

  new_X: shape(n, 2, Tx)
  new_y: shape(n,)
  n is the new number of samples , which could be different that the original (m)
  n = num_pos_neg * unique_classes
  """

  X = X.copy()
  Y = Y.copy()

  # -1 for padded values
  indices_X = pre_process_strings(X, Tx, char_to_index)
  indices_y = pre_process_strings(Y, Tx, char_to_index)

  start_new_category = []
  for i, sentence in enumerate(Y):
    # maintain the starting index of a new category
    if i == 0:
      start_new_category.append(i)
    else:
      if Y[i] != Y[i - 1]:
        start_new_category.append(i)


  # make positive and negative samples
  n = (num_pos_neg * 2 * unique_classes)
  new_X = np.zeros((n, 2, Tx))
  new_y = np.full((n,), -1)
  j = 0
  i = 0
  count = 0
  while(True):

    if count == n:
      break

    if count % (num_pos_neg * 2) == 0 and count != 0:
      j += 1
      if j == len(start_new_category) - 1:
        break
      i = start_new_category[j]

    new_X[count, 0, :] = indices_X[i, :]
    new_X[count, 1, :] = indices_y[i, :]
    new_y[count] = 1

    # random index for negative samples (not similar)
    rand_index = np.random.randint(low= 0, high= X.shape[0], size=1)[0]
    # if the random index is the same category as current category, get another index
    while np.array_equal(indices_y[rand_index, :], indices_y[i, :]):
      rand_index = np.random.randint(low= 0, high= X.shape[0], size=1)[0]

    new_X[count + 1, 0, :] = indices_X[i + 1, :]
    new_X[count + 1, 1, :] = indices_y[rand_index, :]
    new_y[count + 1] = 0

    i += 1
    count += 2

  return new_X, new_y

def pre_process_input(sentence1, sentence2, Tx, char_to_index):
  """
  sentence1: string
  sentence2: string
  Tx: sequence length
  char_to_index: a dict mapping all characters in vocab to their indices (0 to vocab_size)

  Returns:
    x: shape(1, 2, Tx, len(char_to_index)) , an input ready for prediction
  """

  sentences = [sentence1, sentence2]
  x = pre_process_strings(sentences, Tx, char_to_index)[np.newaxis, ...]

  return tf.one_hot(x, depth=len(char_to_index), axis=-1)

unique_categ = len(sku_values.unique())
X, y = pre_process_dataset_training(data["seller_item_name"], data["marketplace_product_name_ar"],
                       Tx, char_to_index, unique_categ)

big = 0

for index in X[big, 0, :]:
  if index == -1:
    continue
  print(index_to_char[index], end="")

print("\n")
for index in X[big, 1, :]:
  if index == -1:
    continue
  print(index_to_char[index], end="")

print("\n")
print(y[big])

print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

X_train = tf.one_hot(X_train, depth = vocab_size, axis = -1)

X_test =  tf.one_hot(X_test, depth = vocab_size, axis = -1)

"""- The input data X : consists of a tuple (inp1, inp2)
  - Each input represents a sentence (medication name)

-  The target y : is 0 or 1 for similar or dissimilar, a binary classification problem.

- The architecture is (for each input, shared weights) a masking layer , LSTM layer, a distance function which computes the euclidean distance between the final hidden states of the two lstms then Dense Layer with sigmoid activation.
"""

class Model(tf.keras.Model):
  def __init__(self, num_of_layers, hidden_dim ,dropout_rate = 0.3, **kwargs):
    super(Model, self).__init__(**kwargs)
    self.num_layers = num_of_layers
    self.hidden_dim = hidden_dim
    self.dropout_rate = dropout_rate
    self.masking = Masking(mask_value = 0)
    self.last_lstm = LSTM(hidden_dim, return_state = True)
    self.lstm_layers = [LSTM(hidden_dim, return_sequences=True)
                        for _ in range(self.num_layers - 1)]
    self.dropout = Dropout(dropout_rate)
    self.dense = Dense(1, activation="sigmoid")

  def call_once(self, x):
    x = self.masking(x)

    for i in range(len(self.lstm_layers)):
      x = self.lstm_layers[i](x)
      x = self.dropout(x)

    x, final_hidden, _ = self.last_lstm(x)

    return final_hidden

  def call(self, inputs):
    x1 = inputs[:, 0, :, :]
    x2 = inputs[:, 1, :, :]

    first_output = self.call_once(x1)
    sec_output = self.call_once(x2)

    distance = tf.norm(first_output - sec_output, axis =1 , keepdims=True)
    distance = tf.clip_by_value(distance, clip_value_min=1e-7, clip_value_max=1e7)

    return self.dense(distance)

  def get_config(self):
    # Return the configuration of the custom layer, including any parameters
    config = super(Model, self).get_config()
    # Add any custom parameters here
    config.update({
        "num_of_layers":self.num_layers,
        "hidden_dim": self.hidden_dim,
        "dropout_rate" : self.dropout_rate
    })
    return config

model = Model(1, 64)
# (1, 64) got inference of batch size of 32 in 12ms

print(X_train.shape)

opt = tf.keras.optimizers.Adam(learning_rate=0.005)
model.compile(optimizer=opt, loss="binary_crossentropy", metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs= 4, verbose=1, batch_size=64)

import matplotlib.pyplot as plt

plt.plot(history.history["loss"])

preds = model.predict(X_test, batch_size=64)
binary_preds = (preds > 0.5).astype(np.float32).squeeze()
# the model maximum must process a 32 batch size in 15 ms

print(accuracy_score(y_test,binary_preds))

def get_prediction(model, input_sentences, Tx, char_to_index ,file):
  """
  input_sentences: array of strings of medication names
  file: string of file path containing the targets (master file)

  get prediction using 'input_sentence' from the master file

  Returns:
    matched name, probability
  """

  targets = pd.read_excel(file, sheet_name = 'Master File')["product_name_ar"]

  # shape (targets[0], Tx)
  targets_indices = np.expand_dims(pre_process_strings(targets, Tx, char_to_index), 1)

  input_indices = pre_process_strings(input_sentences, Tx, char_to_index)

  # repeat input to try to match it with each sentence in targets
  input_repeated = np.expand_dims(np.repeat(input_indices, targets.shape[0], axis=0), 1)

  # targets_indices_repeated = np.concatenate([targets_indices, targets_indices], axis=0)

  targets_indices_repeated = np.tile(targets_indices, (len(input_sentences), 1, 1))

  concat_indices = np.concatenate((targets_indices_repeated, input_repeated), axis=1)

  model_input = tf.one_hot(concat_indices, depth =len(char_to_index), axis=-1)

  preds = model.predict(model_input, batch_size=1024).reshape(len(input_sentences), targets.shape[0])

  # index of the biggest probab
  index = np.argmax(preds, axis = 1)

  return pd.DataFrame({
      "Probability": np.max(preds, 1),
      "Matched": targets[index].values,
      "Input":input_sentences,

  })

  # return np.max(preds, 1), targets[index]

sen = "الفانوفا"
sen2 = "الفانوفا بلس"
sen3 = "  بانادول اكسترا  48"
sen4 = "اسبوسيد اطفال 75"
sen5 = " مش موجود جديييد"

get_prediction(model, np.array([sen,  sen2, sen3, sen4, sen5]), Tx, char_to_index, "data.xlsx")

# x_try_1 = pre_process_input("اسبوسيد اطفال 75 مجم 30 قرص مضغ", "كارنيفيتا ادفانس للنساء", Tx, char_to_index)

# x_try_2 = pre_process_input("داونوبرازول20ملى قرص جديييييييييييد ", "  داونوبرازول 20/1100 مجم 14 كبسول", Tx, char_to_index)

# x_try_3 = pre_process_input("الفانوفا 0.15% قطرة عين 5 مل", "الفانوفا بلس قطرة ", Tx, char_to_index)

# x_try_4 = pre_process_input("ESTOHALT 40 MG 14 CAP", "استوهالت 40 مجم 14 كبسول", Tx, char_to_index)

# x_try_5 = pre_process_input( "اماريل 3 مجم 30 قرص", "AMARYL TABS 3 MG 30", Tx, char_to_index)

# x_try = tf.concat([x_try_1, x_try_2, x_try_3, x_try_4, x_try_5], 0)

# preds = model.predict(x_try)
# print((preds * 100))

